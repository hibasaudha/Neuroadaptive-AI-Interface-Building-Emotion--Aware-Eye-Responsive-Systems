import re
from typing import Tuple

# --- VISUAL MODEL CONSTANTS (Based on your original combined model code) ---
# Your EMOTION_CLASSES were: ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral", "stress"]
# Your EYE_CLASSES were: ['close_look', 'forward_look', 'left_look', 'right_look']

# --- COMMUNICATION FILE ---
OUTPUT_FILE = "cv_output.txt" 
# This file is created and updated by cv_server.py

def read_cv_state() -> Tuple[str, str]:
    """
    Reads the latest Fused Visual State from the file generated by cv_server.py.
    This simulates real-time Inter-Process Communication (IPC).
    Returns: (face_emotion_label, eye_movement_label)
    """
    try:
        with open(OUTPUT_FILE, 'r') as f:
            # File format is: EMOTION_LABEL|EYE_LABEL
            data = f.read().strip()
            if '|' in data:
                face_emotion, eye_movement = data.split('|')
                return face_emotion, eye_movement
            
    except FileNotFoundError:
        # File not found means the CV server hasn't started yet
        print(f"  [IPC ERROR] '{OUTPUT_FILE}' not found. Is cv_server.py running?")
        pass # Fall through to default return

    return "N/A", "N/A" # Default state if reading fails or file is not ready

# --- TEXT ANALYTICS (MOCK TRANSFORMER) ---

def get_transformer_sentiment(text_input: str) -> str:
    """
    MOCK FUNCTION: Simulates using a Hugging Face Transformer pipeline 
    to classify the user's text into a distinct emotion label.
    
    Labels used: 'SADNESS', 'FEAR', 'ANGER', 'LONELINESS', 'JOY', 'NEUTRAL'
    """
    lower_text = text_input.lower()
    
    if re.search(r'\b(sad|down|miserable|disappointed|lost|heartbroken)\b', lower_text): 
        return 'SADNESS' 
    if re.search(r'\b(stressed|anxious|worry|nervous|overwhelmed|panic|scared)\b', lower_text): 
        return 'FEAR' 
    if re.search(r'\b(angry|frustrated|irritated|mad|terrible|hate|annoyed)\b', lower_text): 
        return 'ANGER' 
    if re.search(r'\b(alone|lonely|nobody|no friends|isolated)\b', lower_text): 
        return 'LONELINESS' 
    if re.search(r'\b(happy|great|good|fine|okay|awesome|excellent|excited)\b', lower_text): 
        return 'JOY' 

    return 'NEUTRAL'


def get_realtime_face_emotion() -> str:
    """ Fetches the face emotion label from the live CV output. """
    face_emotion, _ = read_cv_state()
    return face_emotion
    
def get_realtime_eye_movement() -> str:
    """ Fetches the eye movement label from the live CV output. """
    _, eye_movement = read_cv_state()
    return eye_movement


def simulate_visual_models(text_input: str) -> Tuple[str, str]:
    """
    Combines real-time face and eye input (read from the CV server's output file).
    
    Returns: (real_time_face_emotion, real_time_eye_movement)
    """
    simulated_face_emotion = get_realtime_face_emotion()
    simulated_eye_movement = get_realtime_eye_movement()
    
    return simulated_face_emotion, simulated_eye_movement


# --- MULTIMODAL FUSION LOGIC (The crucial output function) ---

def get_fused_emotion(text_input: str) -> str:
    """
    Applies the FUSION DECISION RULES using inputs from three models (Text, Face, Eye).
    Returns the single, definitive FUSED emotion state.
    """
    text_emotion = get_transformer_sentiment(text_input)
    # NOTE: The visual models now read directly from the live output of cv_server.py
    simulated_face_emotion, simulated_eye_movement = simulate_visual_models(text_input)
    
    print(f"  [DEBUG] Real-Time Face Emotion: {simulated_face_emotion} (From CV Server)")
    print(f"  [DEBUG] Transformer Text Emotion: {text_emotion}")
    print(f"  [DEBUG] Real-Time Eye Movement: {simulated_eye_movement} (From CV Server)")
    
    # --- FUSION DECISION RULES using your specific CV labels ---
    
    # RULE 1: High alert - Text is sad (SADNESS/LONELINESS) AND visual cues show withdrawal
    if (text_emotion in ('SADNESS', 'LONELINESS') and 
        simulated_eye_movement == 'close_look' and 
        simulated_face_emotion == 'sad'):
        return 'Deep Sadness/Withdrawal' # Needs maximum empathy
    
    # RULE 2: Hidden distress - Text is neutral (NEUTRAL) but face shows stress/fear
    if text_emotion == 'NEUTRAL' and simulated_face_emotion in ('stress', 'fear'):
        return 'Hidden Anxiety' # Chatbot needs to gently probe

    # RULE 3: Frustration with agitation - Text is angry/fear (ANGER/FEAR) AND eyes are darting 
    if (text_emotion in ('ANGER', 'FEAR') and 
        simulated_eye_movement in ('left_look', 'right_look') and
        simulated_face_emotion in ('angry', 'fear')):
         return 'Frustrated/Agitated'
    
    # Default: If no fusion rules apply, map the Transformer label to a conversational state
    if text_emotion == 'SADNESS' or text_emotion == 'LONELINESS':
         return 'General Sadness'
    if text_emotion == 'FEAR':
         return 'High Anxiety'
    if text_emotion == 'ANGER':
         return 'Irritation'
    if text_emotion == 'JOY' or simulated_face_emotion == 'happy':
         return 'Excited/Positive'

    return 'Neutral'
